<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The 140x Problem | AI Disclosure Analysis</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Helvetica Neue', -apple-system, BlinkMacSystemFont, sans-serif;
            background: #0a0a0a;
            color: #e0e0e0;
            line-height: 1.6;
            overflow-x: hidden;
        }

        /* Zaha Hadid Principle: Flowing background with parametric curves */
        body::before {
            content: '';
            position: fixed;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: 
                radial-gradient(ellipse at 20% 30%, rgba(99, 102, 241, 0.15) 0%, transparent 50%),
                radial-gradient(ellipse at 80% 70%, rgba(236, 72, 153, 0.15) 0%, transparent 50%),
                radial-gradient(ellipse at 50% 50%, rgba(139, 92, 246, 0.1) 0%, transparent 70%);
            animation: flow 30s ease-in-out infinite;
            z-index: -1;
        }

        @keyframes flow {
            0%, 100% { transform: translate(0, 0) rotate(0deg); }
            33% { transform: translate(5%, 10%) rotate(1deg); }
            66% { transform: translate(-5%, -10%) rotate(-1deg); }
        }

        .container {
            max-width: 1800px;
            margin: 0 auto;
            padding: 2rem 1rem;
        }

        /* Zaha Hadid Principle: Asymmetric, flowing header */
        header {
            position: relative;
            padding: 4rem 3rem;
            margin: 2rem 0 4rem;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            clip-path: polygon(0 0, 100% 0, 95% 100%, 5% 100%);
            transform: skewY(-1deg);
            box-shadow: 
                0 25px 50px -12px rgba(0, 0, 0, 0.5),
                inset 0 -1px 0 rgba(255, 255, 255, 0.1);
        }

        header::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(120deg, transparent, rgba(99, 102, 241, 0.1), transparent);
            opacity: 0;
            animation: shine 8s ease-in-out infinite;
        }

        @keyframes shine {
            0%, 100% { opacity: 0; transform: translateX(-100%); }
            50% { opacity: 1; transform: translateX(100%); }
        }

        header > * {
            transform: skewY(1deg);
        }

        h1 {
            font-size: clamp(2rem, 5vw, 3.5rem);
            font-weight: 900;
            margin-bottom: 1rem;
            letter-spacing: -0.03em;
            background: linear-gradient(135deg, #ffffff 0%, #a78bfa 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            font-size: clamp(1rem, 2.5vw, 1.5rem);
            color: #a0aec0;
            font-weight: 300;
            letter-spacing: 0.05em;
        }

        /* Zaha Hadid Principle: Fluid tab navigation with curves */
        .tab-nav {
            display: flex;
            gap: 2rem;
            padding: 0;
            margin: 3rem 0 0;
            position: relative;
        }

        .tab-button {
            flex: 1;
            padding: 1.5rem 2.5rem;
            background: transparent;
            color: #718096;
            border: none;
            cursor: pointer;
            font-size: 1.1rem;
            font-weight: 600;
            letter-spacing: 0.05em;
            transition: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            clip-path: polygon(5% 0%, 95% 0%, 100% 100%, 0% 100%);
        }

        .tab-button::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.1), rgba(139, 92, 246, 0.1));
            opacity: 0;
            transition: opacity 0.4s ease;
        }

        .tab-button:hover {
            color: #e2e8f0;
            transform: translateY(-2px);
        }

        .tab-button:hover::before {
            opacity: 1;
        }

        .tab-button.active {
            color: #ffffff;
            clip-path: polygon(3% 0%, 97% 0%, 100% 100%, 0% 100%);
        }

        .tab-button.active::before {
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.3), rgba(139, 92, 246, 0.3));
            opacity: 1;
        }

        .tab-button.active::after {
            content: '';
            position: absolute;
            bottom: 0;
            left: 5%;
            right: 5%;
            height: 3px;
            background: linear-gradient(90deg, #6366f1, #8b5cf6, #ec4899);
            box-shadow: 0 0 20px rgba(139, 92, 246, 0.5);
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
            animation: fadeSlideIn 0.6s cubic-bezier(0.4, 0, 0.2, 1);
        }

        @keyframes fadeSlideIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }

        /* Zaha Hadid Principle: Asymmetric stat cards with skewed forms */
        .stats-banner {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 2rem;
            padding: 4rem 2rem;
            margin: -2rem 0 4rem;
            position: relative;
        }

        .stat-card {
            padding: 2.5rem;
            background: linear-gradient(135deg, rgba(26, 32, 46, 0.8), rgba(31, 41, 55, 0.6));
            backdrop-filter: blur(20px);
            border: 1px solid rgba(255, 255, 255, 0.1);
            clip-path: polygon(8% 0%, 100% 0%, 92% 100%, 0% 100%);
            transform-origin: center;
            transition: all 0.5s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }

        .stat-card::before {
            content: '';
            position: absolute;
            top: -50%;
            left: -50%;
            width: 200%;
            height: 200%;
            background: linear-gradient(45deg, transparent, rgba(99, 102, 241, 0.1), transparent);
            transform: rotate(45deg);
            transition: transform 0.6s ease;
        }

        .stat-card:hover {
            transform: translateY(-8px) scale(1.02);
            border-color: rgba(139, 92, 246, 0.3);
            box-shadow: 
                0 25px 50px -12px rgba(139, 92, 246, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .stat-card:hover::before {
            transform: rotate(45deg) translateX(50%);
        }

        .stat-number {
            font-size: 3.5rem;
            font-weight: 900;
            background: linear-gradient(135deg, #fff 0%, #a78bfa 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 1rem;
            color: #9ca3af;
            font-weight: 500;
            letter-spacing: 0.05em;
            text-transform: uppercase;
        }

        .content {
            padding: 2rem;
        }

        .section {
            margin-bottom: 6rem;
            position: relative;
        }

        h2 {
            font-size: 2.5rem;
            font-weight: 800;
            margin-bottom: 2rem;
            background: linear-gradient(135deg, #ffffff 0%, #8b5cf6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            letter-spacing: -0.02em;
        }

        h3 {
            font-size: 1.75rem;
            font-weight: 700;
            margin: 3rem 0 1.5rem;
            color: #e2e8f0;
        }

        p {
            color: #cbd5e0;
            line-height: 1.8;
        }

        /* Zaha Hadid Principle: Curved, flowing insight boxes */
        .insight-box {
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.1) 0%, rgba(139, 92, 246, 0.1) 100%);
            border-left: 4px solid #8b5cf6;
            padding: 2rem;
            margin: 2rem 0;
            border-radius: 0 30px 30px 0;
            clip-path: polygon(0 0, 100% 0, 98% 100%, 0 100%);
            box-shadow: 0 10px 30px rgba(139, 92, 246, 0.1);
            transition: all 0.4s ease;
        }

        .insight-box:hover {
            transform: translateX(5px);
            box-shadow: 0 15px 40px rgba(139, 92, 246, 0.2);
        }

        .insight-title {
            font-weight: 700;
            color: #a78bfa;
            margin-bottom: 0.75rem;
            font-size: 1.1rem;
            letter-spacing: 0.05em;
        }

        /* Zaha Hadid Principle: Dynamic, asymmetric chart containers */
        .chart-container {
            position: relative;
            height: 450px;
            margin: 3rem 0;
            padding: 2rem;
            background: linear-gradient(135deg, rgba(26, 32, 46, 0.6), rgba(31, 41, 55, 0.4));
            backdrop-filter: blur(20px);
            border: 1px solid rgba(255, 255, 255, 0.05);
            border-radius: 40px;
            clip-path: polygon(2% 0%, 98% 0%, 100% 98%, 0% 100%);
            box-shadow: 
                0 25px 50px -12px rgba(0, 0, 0, 0.5),
                inset 0 1px 0 rgba(255, 255, 255, 0.03);
            overflow: hidden;
            transition: all 0.5s cubic-bezier(0.4, 0, 0.2, 1);
        }

        .chart-container::before {
            content: '';
             position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(135deg, rgba(99, 102, 241, 0.05), transparent);
            pointer-events: none;
        }

        .chart-container:hover {
            transform: translateY(-5px);
            box-shadow: 
                0 35px 60px -12px rgba(139, 92, 246, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.05);
        }

        /* Zaha Hadid Principle: Flowing table design */
        .comparison-table {
            width: 100%;
            border-collapse: separate;
            border-spacing: 0;
            margin: 3rem 0;
            border-radius: 30px;
            overflow: hidden;
            background: rgba(26, 32, 46, 0.6);
            backdrop-filter: blur(20px);
            box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
        }

        .comparison-table thead {
            background: linear-gradient(135deg, #1f2937 0%, #111827 100%);
        }

        .comparison-table th {
            padding: 1.5rem;
            text-align: left;
            font-weight: 700;
            color: #a78bfa;
            letter-spacing: 0.05em;
            text-transform: uppercase;
            font-size: 0.9rem;
        }

        .comparison-table td {
            padding: 1.5rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
            color: #cbd5e0;
        }

        .comparison-table tbody tr {
            transition: all 0.3s ease;
        }

        .comparison-table tbody tr:hover {
            background: rgba(139, 92, 246, 0.1);
            transform: translateX(5px);
        }

        .badge {
            display: inline-block;
            padding: 0.4rem 1rem;
            border-radius: 20px;
            font-size: 0.85rem;
            font-weight: 700;
            letter-spacing: 0.03em;
        }

        .badge-critical {
            background: rgba(239, 68, 68, 0.2);
            color: #fca5a5;
            border: 1px solid rgba(239, 68, 68, 0.3);
        }

        .badge-warning {
            background: rgba(245, 158, 11, 0.2);
            color: #fcd34d;
            border: 1px solid rgba(245, 158, 11, 0.3);
        }

        .badge-good {
            background: rgba(16, 185, 129, 0.2);
            color: #6ee7b7;
            border: 1px solid rgba(16, 185, 129, 0.3);
        }

        /* Zaha Hadid Principle: Asymmetric pattern cards */
        .pattern-card {
            background: linear-gradient(135deg, rgba(26, 32, 46, 0.8), rgba(31, 41, 55, 0.6));
            backdrop-filter: blur(20px);
            padding: 3rem;
            margin: 2rem 0;
            clip-path: polygon(3% 0%, 100% 0%, 97% 100%, 0% 100%);
            border-left: 4px solid #8b5cf6;
            box-shadow: 
                0 20px 40px rgba(0, 0, 0, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.05);
            transition: all 0.5s cubic-bezier(0.4, 0, 0.2, 1);
            position: relative;
            overflow: hidden;
        }

        .pattern-card::before {
            content: '';
            position: absolute;
            top: -100%;
            right: -100%;
            width: 200%;
            height: 200%;
            background: radial-gradient(circle, rgba(139, 92, 246, 0.1) 0%, transparent 70%);
            transition: transform 0.8s ease;
        }

        .pattern-card:hover {
            transform: translateX(10px) scale(1.01);
            border-left-color: #ec4899;
            box-shadow: 
                0 30px 60px rgba(139, 92, 246, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .pattern-card:hover::before {
            transform: translate(-50%, -50%);
        }

        .pattern-name {
            font-size: 2rem;
            font-weight: 900;
            background: linear-gradient(135deg, #a78bfa 0%, #ec4899 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 1rem;
            letter-spacing: -0.02em;
        }

        .pattern-stat {
            font-size: 4rem;
            font-weight: 900;
            color: #fff;
            margin: 1.5rem 0;
            text-shadow: 0 0 30px rgba(139, 92, 246, 0.5);
        }

        /* Paper tab styling */
        .paper-content {
            max-width: 1000px;
            margin: 0 auto;
            padding: 4rem 3rem;
            line-height: 1.9;
        }

        .paper-content h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
            background: linear-gradient(135deg, #ffffff 0%, #a78bfa 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .paper-content h2 {
            font-size: 2.25rem;
            margin: 4rem 0 1.5rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid rgba(139, 92, 246, 0.3);
        }

        .paper-content h3 {
            font-size: 1.75rem;
            margin: 3rem 0 1.5rem;
        }

        .paper-content p {
            margin: 1.5rem 0;
        }

        .paper-content strong {
            color: #e2e8f0;
            font-weight: 600;
        }

        .paper-content table {
            width: 100%;
            margin: 3rem 0;
            border-collapse: collapse;
            background: rgba(26, 32, 46, 0.4);
            border-radius: 20px;
            overflow: hidden;
        }

        .paper-content table th {
            background: rgba(31, 41, 55, 0.8);
            padding: 1rem;
            text-align: left;
            font-weight: 600;
            border-bottom: 2px solid rgba(139, 92, 246, 0.3);
            color: #a78bfa;
        }

        .paper-content table td {
            padding: 1rem;
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
        }

        .paper-content ul, .paper-content ol {
            margin-left: 2rem;
            margin: 1.5rem 0;
        }

        .paper-content li {
            margin: 0.75rem 0;
        }

        .paper-content hr {
            border: none;
            border-top: 2px solid rgba(139, 92, 246, 0.2);
            margin: 4rem 0;
        }

        footer {
            background: linear-gradient(135deg, #111827 0%, #1f2937 100%);
            color: #9ca3af;
            padding: 3rem 2rem;
            text-align: center;
            margin-top: 6rem;
            clip-path: polygon(5% 0%, 100% 0%, 95% 100%, 0% 100%);
        }

        .grid-2 {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(450px, 1fr));
            gap: 3rem;
            margin: 3rem 0;
        }

        @media (max-width: 768px) {
            header {
                clip-path: polygon(0 0, 100% 0, 98% 100%, 2% 100%);
            }

            .grid-2 {
                grid-template-columns: 1fr;
            }
            
            .chart-container {
                height: 350px;
            }

            .paper-content {
                padding: 2rem 1.5rem;
            }

            .stat-card {
                clip-path: polygon(5% 0%, 100% 0%, 95% 100%, 0% 100%);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>The 140x Problem</h1>
            <div class="subtitle">Why AI Companies Disclose Infrastructure Risks 140 Times More Than Model Risks</div>
            
            <div class="tab-nav">
                <button class="tab-button active" onclick="switchTab('dashboard')">
                    INTERACTIVE DATA
                </button>
                <button class="tab-button" onclick="switchTab('paper')">
                    RESEARCH PAPER
                </button>
                <button class="tab-button" onclick="switchTab('downloads')">
                    DOWNLOADS
                </button>
            </div>
        </header>

        <div id="dashboard-tab" class="tab-content active">
            <div class="stats-banner">
                <div class="stat-card">
                    <div class="stat-number">1,876</div>
                    <div class="stat-label">Companies Analyzed</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">654</div>
                    <div class="stat-label">With AI Disclosures</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">6,878</div>
                    <div class="stat-label">AI-Related Mentions</div>
                </div>
                <div class="stat-card">
                    <div class="stat-number">140x</div>
                    <div class="stat-label">Disclosure Ratio Gap</div>
                </div>
            </div>

            <div style="text-align: center; margin: 2rem 0 4rem;">
                <a href="140x_Problem_Key_Findings.csv" download class="download-btn" style="display: inline-block; padding: 1rem 2.5rem; background: linear-gradient(135deg, #6366f1, #8b5cf6); color: white; text-decoration: none; border-radius: 30px; font-weight: 700; letter-spacing: 0.05em; text-transform: uppercase; box-shadow: 0 10px 30px rgba(99, 102, 241, 0.4); transition: all 0.3s ease;">
                    â¬‡ Download Key Findings Data (CSV)
                </a>
            </div>

            <div class="content">
                <div class="section">
                    <h2>Executive Findings</h2>
                    <p style="font-size: 1.2rem; margin-bottom: 3rem;">
                        We extracted and analyzed every available SEC 10-K Item 1A (Risk Factors) filingâ€”1,876 companies totalâ€”and discovered a systematic pattern: companies are advertising AI capabilities while concealing AI-specific risks.
                    </p>

                    <div class="pattern-card">
                        <div class="pattern-name">The LLM Paradox</div>
                        <div class="pattern-stat">1,038 : &lt;10</div>
                        <p>1,038 companies mention "LLM" in risk factors, but fewer than 10 mention "hallucination"â€”the technology's most distinctive risk.</p>
                    </div>

                    <div class="pattern-card">
                        <div class="pattern-name">The 140x Rule</div>
                        <div class="pattern-stat">140 : 1</div>
                        <p>For every mention of model performance risk, there are 140 mentions of infrastructure riskâ€”backwards for AI application companies.</p>
                    </div>

                    <div class="pattern-card">
                        <div class="pattern-name">The NVIDIA Proxy</div>
                        <div class="pattern-stat">56 vs &lt;10</div>
                        <p>56 companies name NVIDIA (vendor concentration risk) while &lt;10 mention hallucination (product liability risk).</p>
                    </div>
                </div>

                <div class="section">
                    <h2>Disclosure by Risk Category</h2>
                    <div class="chart-container">
                        <canvas id="disclosureChart"></canvas>
                    </div>
                </div>

                <div class="section">
                    <h2>The Disclosure Inversion</h2>
                    <div class="grid-2">
                        <div class="chart-container">
                            <canvas id="infrastructureChart"></canvas>
                        </div>
                        <div class="chart-container">
                            <canvas id="modelRiskChart"></canvas>
                        </div>
                    </div>

                    <div class="insight-box">
                        <div class="insight-title">KEY INSIGHT</div>
                        <p>Companies are comfortable disclosing risks they don't control (supply chain, vendors) while remaining silent on risks they do control but can't eliminate (model performance, training data, bias).</p>
                    </div>
                </div>

                <div class="section">
                    <h2>AI Risk Categories: Academic Standards vs. Current Disclosure</h2>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Risk Category</th>
                                <th>Academic Priority</th>
                                <th>Current Disclosure Rate</th>
                                <th>Gap Assessment</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>Model Performance & Reliability</strong></td>
                                <td>High</td>
                                <td>&lt;1%</td>
                                <td><span class="badge badge-critical">Critical Gap</span></td>
                            </tr>
                            <tr>
                                <td><strong>Training Data & IP</strong></td>
                                <td>High</td>
                                <td>&lt;1% (17 mentions)</td>
                                <td><span class="badge badge-critical">Critical Gap</span></td>
                            </tr>
                            <tr>
                                <td><strong>Bias & Fairness</strong></td>
                                <td>High</td>
                                <td>9% (214 mentions)</td>
                                <td><span class="badge badge-warning">Significant Gap</span></td>
                            </tr>
                            <tr>
                                <td><strong>Regulatory Compliance</strong></td>
                                <td>High</td>
                                <td>7% (EU AI Act: 46 mentions)</td>
                                <td><span class="badge badge-critical">Critical Gap</span></td>
                            </tr>
                            <tr>
                                <td><strong>Privacy & Data Protection</strong></td>
                                <td>High</td>
                                <td>~30%</td>
                                <td><span class="badge badge-warning">Moderate Gap</span></td>
                            </tr>
                            <tr>
                                <td><strong>Security & Adversarial Attacks</strong></td>
                                <td>High</td>
                                <td>~30%</td>
                                <td><span class="badge badge-warning">Moderate Gap</span></td>
                            </tr>
                            <tr>
                                <td><strong>Infrastructure Dependencies</strong></td>
                                <td>Medium</td>
                                <td>58%</td>
                                <td><span class="badge badge-good">Well Covered</span></td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <div class="section">
                    <h2>Keyword Frequency Analysis</h2>
                    <div class="chart-container" style="height: 550px;">
                        <canvas id="keywordChart"></canvas>
                    </div>
                </div>

                <div class="section">
                    <h2>Disclosure Depth Analysis</h2>
                    <div class="chart-container">
                        <canvas id="depthChart"></canvas>
                    </div>

                    <div class="insight-box">
                        <div class="insight-title">QUALITY VS QUANTITY</div>
                        <p>63% of companies mentioning AI provide only minimal disclosure (â‰¤5 mentions). Only 2.3% (15 companies) provide comprehensive disclosure (50+ mentions).</p>
                    </div>
                </div>

                <div class="section">
                    <h2>Cross-Jurisdictional Comparison</h2>
                    <table class="comparison-table">
                        <thead>
                            <tr>
                                <th>Jurisdiction</th>
                                <th>Disclosure Standard</th>
                                <th>Enforcement Level</th>
                                <th>Competitive Implication</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>United States (SEC)</strong></td>
                                <td>Flexible, principles-based</td>
                                <td>Weak (for now)</td>
                                <td>Race to the bottomâ€”minimal disclosure</td>
                            </tr>
                            <tr>
                                <td><strong>European Union</strong></td>
                                <td>Mandatory (AI Act)</td>
                                <td>Strong (6% of revenue)</td>
                                <td>Highest global standardâ€”signals governance quality</td>
                            </tr>
                            <tr>
                                <td><strong>United Kingdom</strong></td>
                                <td>Model governance framework</td>
                                <td>Sector-specific</td>
                                <td>Technical detail requiredâ€”proves feasibility</td>
                            </tr>
                            <tr>
                                <td><strong>China</strong></td>
                                <td>Algorithm registration</td>
                                <td>Strong</td>
                                <td>Granular disclosure mandated</td>
                            </tr>
                            <tr>
                                <td><strong>Singapore</strong></td>
                                <td>FEAT principles (soft law)</td>
                                <td>Reputational</td>
                                <td>Competitive signaling in quality markets</td>
                            </tr>
                            <tr>
                                <td><strong>Canada</strong></td>
                                <td>AIDA (Artificial Intelligence and Data Act)</td>
                                <td>Emerging (legislative process)</td>
                                <td>North American leadership positioningâ€”ethical AI disclosure premium</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </div>
        </div>

        <div id="paper-tab" class="tab-content">
            <div class="paper-content" id="paper-rendered"></div>
        </div>

        <div id="downloads-tab" class="tab-content">
            <div class="paper-content">
                <h2 style="text-align: center; margin-bottom: 1rem;">Downloads & Resources</h2>
                <p style="text-align: center; color: #9ca3af; margin-bottom: 3rem;">
                    <strong>Data As Of:</strong> December 31, 2025 | <strong>SEC EDGAR Full-Text Search</strong> | <strong>Filing Period:</strong> FY 2024-2025
                </p>

                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 2rem; margin: 3rem 0;">
                    
                    <div class="pattern-card" style="text-align: center;">
                        <div class="pattern-name" style="font-size: 1.5rem;">ðŸ“„ Research Paper</div>
                        <p style="margin: 1.5rem 0;">Complete analysis with methodology, findings, cross-jurisdictional comparison, and sample S-1 disclosure templates.</p>
                        <a href="S1_AI_Risk_Disclosure_Liability_Analysis.md" download style="display: inline-block; padding: 1rem 2rem; background: linear-gradient(135deg, #6366f1, #8b5cf6); color: white; text-decoration: none; border-radius: 25px; font-weight: 600;">
                            Download (Markdown)
                        </a>
                    </div>

                    <div class="pattern-card" style="text-align: center;">
                        <div class="pattern-name" style="font-size: 1.5rem;">ðŸ“Š Key Findings Data</div>
                        <p style="margin: 1.5rem 0;">Complete dataset of empirical findings, keyword frequencies, disclosure gaps, and cross-jurisdictional requirements.</p>
                        <a href="140x_Problem_Key_Findings.csv" download style="display: inline-block; padding: 1rem 2rem; background: linear-gradient(135deg, #10b981, #059669); color: white; text-decoration: none; border-radius: 25px; font-weight: 600;">
                            Download (CSV)
                        </a>
                    </div>

                    <div class="pattern-card" style="text-align: center;">
                        <div class="pattern-name" style="font-size: 1.5rem;">ðŸ“š References</div>
                        <p style="margin: 1.5rem 0;">Chicago-style bibliography of all sources including regulatory frameworks, academic research, and litigation.</p>
                        <a href="140x_Problem_References.md" download style="display: inline-block; padding: 1rem 2rem; background: linear-gradient(135deg, #f59e0b, #d97706); color: white; text-decoration: none; border-radius: 25px; font-weight: 600;">
                            Download (Markdown)
                        </a>
                    </div>

                </div>

                <div class="insight-box" style="margin-top: 4rem;">
                    <div class="insight-title">DATA METHODOLOGY</div>
                    <p><strong>Source:</strong> SEC EDGAR Full-Text Search API</p>
                    <p><strong>Filing Type:</strong> Form 10-K (Annual Reports)</p>
                    <p><strong>Section Analyzed:</strong> Item 1A (Risk Factors)</p>
                    <p><strong>Total Companies:</strong> 1,876 (complete universe of available filings)</p>
                    <p><strong>Analysis Date:</strong> December 2025</p>
                    <p><strong>Keywords Tracked:</strong> 30+ AI-related terms across 7 risk categories</p>
                </div>
            </div>
        </div>

        <footer>
            <p><strong>Prepared by:</strong> Tanya Matanda | <strong>Date:</strong> January 2026</p>
            <p style="margin-top: 0.75rem; font-size: 0.9rem;">Parametric analysis for informational purposes only.</p>
        </footer>
    </div>

    <script>
        // Tab switching
        function switchTab(tabName) {
            document.querySelectorAll('.tab-content').forEach(tab => {
                tab.classList.remove('active');
            });
            document.querySelectorAll('.tab-button').forEach(btn => {
                btn.classList.remove('active');
            });

            if (tabName === 'dashboard') {
                document.getElementById('dashboard-tab').classList.add('active');
                document.querySelector('[onclick*="dashboard"]').classList.add('active');
            } else if (tabName === 'paper') {
                document.getElementById('paper-tab').classList.add('active');
                document.querySelector('[onclick*="paper"]').classList.add('active');
                
                if (document.getElementById('paper-rendered').innerHTML === '') {
                    renderPaper();
                }
            } else if (tabName === 'downloads') {
                document.getElementById('downloads-tab').classList.add('active');
                document.querySelector('[onclick*="downloads"]').classList.add('active');
            }
        }

        function renderPaper() {
            const paperMarkdown = "# The 140x Problem: Why AI Companies Disclose Infrastructure Risks 140 Times More Than Model Risks\n## An Empirical Analysis of 1,876 SEC Filings and What It Means for Boards\n\n*An Analysis for Directors, Investors & Corporate Stakeholders*\n\n\ud83d\udcca **[View Interactive Dashboard](https://tanyamatanda.github.io/deep-tech-2026-report/AI_Disclosure_Gap_Dashboard.html)** | Explore the data with dynamic visualizations and charts\n\n---\n\n## Executive Summary: The Disclosure Inversion\n\nWe analyzed all 1,876 available SEC 10-K filings and discovered that companies are disclosing infrastructure risks at **140 times the rate** of model performance risks\u2014a disclosure inversion that creates massive governance exposure for boards and misinformation risk for investors.\n\nOur empirical analysis reveals three distinct patterns that every director and investor should understand:\n\n**The LLM Paradox**: 1,038 companies advertise their use of large language models in risk factors, but fewer than 10 mention \"hallucination\"\u2014the technology's most distinctive risk. Companies are marketing AI capabilities while concealing AI-specific vulnerabilities.\n\n**The 140x Rule**: For every single mention of model performance risk (accuracy, hallucination, bias), there are 140 mentions of infrastructure risk (compute, chips, vendors). This ratio is backwards for AI application companies, signaling a fundamental failure of risk assessment.\n\n**The NVIDIA Proxy**: Companies comfortably disclose vendor concentration risk (naming NVIDIA 56 times) but remain silent on product liability risk (model failures, bias, IP violations). Directors are approving disclosures that highlight manageable supply chain dependencies while hiding potentially catastrophic product risks.\n\nThese patterns suggest that most companies aren't just failing SEC disclosure requirements\u2014they're failing to exercise basic fiduciary oversight of their AI deployments. This creates three exposures:\n\n- **For directors**: Breach of duty when undisclosed AI risks materialize and shareholders question board oversight\n- **For investors**: Systematic mispricing of AI companies based on incomplete risk information  \n- **For competitive positioning**: Companies maintaining EU-level disclosure standards globally will demonstrate superior governance and capture a disclosure premium\n\nThis analysis provides both the empirical evidence and the cross-jurisdictional framework to help boards distinguish genuine AI governance from regulatory theater.\n\n---\n\n## I. Why This Matters: The Board's Fiduciary Duty in the AI Era\n\n### When Innovation Meets Oversight\n\nWhen a company goes public, its S-1 registration statement serves as both a marketing document and a legal contract with investors. Item 1A\u2014the \"Risk Factors\" section\u2014is where companies must disclose material risks that could affect their business. But here's what makes this critical for boards and investors: adequate risk disclosure isn't just about SEC compliance. It's about whether the company is being honest with itself about the risks it's taking.\n\nFor AI companies in 2026, we've identified a massive gap between the risks these companies actually face and what they're telling investors. This gap represents three distinct problems:\n\n**For directors**: You face potential breach of fiduciary duty if material AI risks aren't disclosed. When those risks materialize\u2014and they will\u2014shareholders will ask why the board allowed inadequate disclosure.\n\n**For investors**: You're making investment decisions based on incomplete information. Companies are showcasing AI as a competitive advantage while systematically concealing AI-specific risks.\n\n**For stakeholders**: The companies you rely on\u2014as customers, partners, or regulators\u2014may be taking undisclosed AI risks that could cascade into your operations.\n\nThe disclosure practices we've uncovered suggest that most companies aren't just failing to meet SEC requirements. They're failing to exercise basic governance over their AI deployments. And that creates both strategic and reputational risk that no legal disclaimer can fix.\n\n---\n\n## II. Our Empirical Findings: What 1,876 Companies Are Actually Disclosing\n\nWe extracted and analyzed the actual Item 1A (Risk Factors) text from every available 10-K filing in the SEC EDGAR database\u20141,876 companies total, representing the complete universe of available public company disclosure data. Using automated text extraction, we scanned for more than thirty AI-related keywords across five risk categories, ultimately identifying 6,878 total AI-related mentions across 654 companies.\n\nWhat we found should concern every board director, investor, and stakeholder evaluating AI companies. The data reveals a systematic pattern: companies are advertising their use of cutting-edge AI technology while systematically failing to disclose the most material risks associated with that technology. The gap between what companies say about AI in their business descriptions and what they disclose in their risk factors represents both a governance failure and a competitive vulnerability that will separate winners from losers as AI regulation and enforcement mature.\n\n---\n\n### The Infrastructure Obsession: Why Everyone's Talking About Chips\n\nNearly six in ten companies that mention AI in their risk factors focus primarily on infrastructure dependencies\u2014compute capacity, data centers, specialized chips, and cloud services. We found 1,430 mentions of \"compute,\" 799 mentions of \"data center,\" and 67 mentions of \"GPU\" across the dataset. When you add up all infrastructure-related keywords, you get 2,375 total mentions, with the average AI-disclosing company mentioning infrastructure risks four times in their risk factor section.\n\nThis makes perfect sense if you're NVIDIA, Amazon Web Services, or a semiconductor manufacturer. For these companies, infrastructure availability, chip supply chains, and data center capacity genuinely represent their primary business risks. But if you're an AI application company\u2014a developer of large language models, an AI-powered SaaS platform, or an autonomous systems provider\u2014and infrastructure dominates your AI risk disclosure, you're materially underrepresenting your actual risk profile. Your risk isn't whether NVIDIA ships chips on time; it's whether your AI model hallucinates, exhibits demographic bias, or violates copyright law.\n\nThe infrastructure obsession reveals something important about current disclosure practices: companies are comfortable disclosing risks they don't control (supply chain, vendor relationships) while remaining silent on risks they do control but can't eliminate (model performance, training data provenance).\n\n---\n\n### The LLM Paradox: Advertising the Technology, Hiding the Risk\n\nThe most striking finding in our analysis is what we call the LLM Paradox. Companies have rapidly adopted terminology related to large language models and generative AI, with 1,038 mentions of \"LLM\" and 446 mentions of \"generative AI\" across the dataset\u2014a combined 1,484 mentions representing a year-over-year explosion in this language compared to prior disclosure periods. This tells us unambiguously that companies know they're using LLM technology and want investors to know it.\n\nBut here's the paradox: fewer than ten companies across the entire 1,876-company dataset mention \"hallucination\"\u2014the single most distinctive and well-documented risk of LLM technology. That's right\u20141,038 companies mention they're using LLMs, but fewer than ten acknowledge that LLMs can generate plausible-looking but factually incorrect outputs.\n\nThis isn't an oversight. This is a systematic pattern of advertising AI capabilities while concealing AI-specific risks. Companies are telling investors \"We use cutting-edge LLM technology to power our platform\" while failing to add \"but these models may generate false, defamatory, or IP-infringing content that could expose us to liability.\" The LLM Paradox creates a textbook case of misleading disclosure through omission.\n\n---\n\n### The Training Data Silence: 99% Aren't Talking About the Elephant in the Courtroom\n\nPerhaps the most stunning disclosure gap we identified concerns training data and intellectual property. Only seventeen companies out of 1,876\u2014less than one percent\u2014mention \"training data\" in their risk factors. Fewer than five mention \"data provenance.\" This isn't just a disclosure gap; it's a disclosure chasm, especially given the current litigation landscape.\n\nWe know from public court filings that the New York Times has sued OpenAI alleging unauthorized use of copyrighted training data. We know Getty Images has sued Stability AI on similar grounds. We know multiple Authors Guild class actions are pending. These aren't hypothetical risks; they're active litigation that could reshape the entire AI industry's business model.\n\nYet 99.1% of companies mentioning AI fail to disclose training data or IP risks. Think about what this means: of the 1,038 companies that explicitly told investors they're using LLMs\u2014models that by definition require massive training datasets\u2014only seventeen acknowledged any risk related to how those models were trained. This ratio is indefensible.\n\nIf you're an AI company going public in 2026, and you're using large language models (whether you developed them or licensed them from third parties), and you trained those models on web-scraped data, and you cannot verify proper licensing for all training content, and you do not disclose this in your S-1, you are in the less-than-one-percent addressing a high-probability, high-severity litigation risk. When the lawsuits come\u2014and they will come\u2014generic \"we may face IP claims\" language will not protect you.\n\n---\n\n### The Bias Disclosure Gap: Two-Thirds Silent on Systematic Risk\n\nOur analysis found that 214 companies mention \"bias\" in connection with AI\u2014which sounds substantial until you realize that represents only 32.7% of the 654 companies disclosing AI risks. Put differently, two-thirds of companies mentioning AI in their risk factors say nothing about bias, despite bias being one of the most widely recognized and heavily regulated AI risks.\n\nThe numbers get worse when you look for more specific language. Fewer than thirty companies mention \"discrimination\" in connection with AI. Fewer than twenty mention \"fairness.\" And fewer than five disclose anything about bias testing methodologies\u2014the actual processes they use to detect and mitigate bias in their systems.\n\nThis silence is particularly alarming given the regulatory context. The EU AI Act explicitly requires bias testing for high-risk AI systems. The EEOC has issued guidance on AI employment tools. HUD, DOJ, and FTC have all signaled AI bias as an enforcement priority. State regulators are following suit. If your AI makes decisions about credit, employment, housing, or insurance, you're operating in a heavily regulated space where bias isn't a hypothetical risk\u2014it's a compliance requirement.\n\nCompanies that deploy AI for these use cases and fail to disclose bias risks aren't just missing an academic talking point. They're failing to disclose material regulatory exposure. When enforcement actions come, and when shareholders sue for inadequate disclosure of that regulatory risk, what's your defense? That everyone else was doing it too?\n\n---\n\n### The Regulatory Compliance Blackout: Only 7% Acknowledge the Law Changed\n\nThe European Union's AI Act became law in 2024, with phased implementation beginning immediately. It represents the most comprehensive AI-specific regulation in the world, imposing risk-based requirements on AI systems deployed in EU markets. High-risk systems\u2014including those used for employment, credit decisioning, biometric identification, and critical infrastructure\u2014face mandatory requirements for transparency, human oversight, bias testing, and documentation. Penalties for non-compliance can reach \u20ac30 million or 6% of global annual revenue, whichever is higher.\n\nGiven the magnitude of this regulatory change, you might expect public companies operating in EU markets to mention it in their risk factors. You would be wrong. Only 46 companies out of the 654 disclosing AI risks\u20147%\u2014mention the EU AI Act specifically. This is particularly shocking because the regulation is not proposed or pending; it's in force. Compliance obligations are accruing right now.\n\nExpand this to all AI-specific regulations\u2014including state laws in California, Colorado, and other jurisdictions\u2014and you still find fewer than 15% of AI-disclosing companies acknowledging regulatory compliance as a material risk. This represents either stunning ignorance or deliberate avoidance. Either way, it creates massive liability exposure.\n\nIf you operate in EU markets, use AI systems that could be classified as \"high-risk\" under the Act, and your S-1 doesn't mention EU AI Act compliance, what exactly is your plan when regulators investigate and shareholders sue for failing to disclose material regulatory obligations? The law is public. The penalties are stated. The disclosure requirement is clear.\n\n---\n\n### The 140x Rule: When Infrastructure Drowns Out Everything Else\n\nWhen we calculated the ratio of infrastructure mentions to model performance risk mentions, we found something striking: for every single mention of model performance risk (hallucination, accuracy limitations, reliability failures), there are 140 mentions of infrastructure risk (compute, data centers, chips, vendors).\n\nLet's be clear about why this ratio matters. For AI infrastructure companies, it's appropriate\u2014infrastructure genuinely is their risk. But for AI application companies, this ratio is completely inverted. If you're building products powered by AI models, your primary risk isn't supply chain. It's model failure. Your risk isn't whether NVIDIA can manufacture enough H100 chips. It's whether your model exhibits bias, generates false outputs, or violates users' rights.\n\nThis 140x ratio gives us a simple litmus test for S-1 review: if your company uses AI as a core product capability and your risk factors mention \"compute\" or \"data center\" before they mention \"model accuracy\"  or \"hallucination,\" your disclosure priorities are backwards. You're spending 140 words talking about vendor relationships and zero words talking about product liability.\n\n---\n\n### The Depth Problem: 63% Provide Only Token Disclosure\n\nBeyond keyword frequency, we analyzed disclosure depth by counting total AI-related mentions per company. What we found suggests that most companies treat AI risk disclosure as a checkbox exercise rather than a substantive analysis. Fully 63% of companies disclosing AI risks mention AI five times or fewer across their entire risk factor section. Only 2.3%\u2014fifteen companies total\u2014provide what we'd call comprehensive disclosure, with fifty or more AI-related mentions.\n\nThis matters because meaningful disclosure requires specificity. You can't adequately warn investors about AI risks with a single paragraph saying \"We use artificial intelligence, which poses risks.\" That's the disclosure equivalent of \"stuff happens\"\u2014it's not specific enough to provide the safe harbor protection that Item 1A is supposed to create.\n\nThe companies providing comprehensive disclosure\u2014IREN with 128 mentions, Ondas Holdings with 107, NVIDIA with 89, Sprinklr with 86, IonQ with 76\u2014aren't just mentioning AI more frequently. They're addressing multiple risk categories with specific, tailored language. They're quantifying where possible. They're describing actual incidents or near-misses. They're treating AI risk disclosure seriously.\n\nFor companies in the 63% providing minimal disclosure, the question is simple: when a material AI-related incident occurs, will your five mentions protect you, or will plaintiffs' counsel argue you only paid lip service to a risk you knew was material?\n\n---\n\n### The NVIDIA Proxy: When Vendor Risk Feels Safer Than Product Risk\n\nOne final pattern worth noting: 56 companies mention NVIDIA by name in their risk factors\u20143% of AI disclosers. Naming a specific supplier is meaningful disclosure. It signals concentration risk, supply chain dependency, pricing exposure, competitive disadvantage if capacity is constrained. Companies making such disclosures are, quite properly, warning investors about vendor concentration.\n\nBut here's what's revealing: companies are more willing to disclose dependence on NVIDIA (a manageable supply risk) than to disclose model hallucination or bias (a potentially catastrophic product liability risk). This isn't because vendor concentration is more material than model failure\u2014it's because vendor concentration feels less threatening to the company's competitive narrative.\n\nThe NVIDIA Proxy reveals the psychology behind these disclosure gaps. Companies want to say \"We're powered by cutting-edge AI\" in their business description. Disclosing infrastructure dependencies doesn't threaten that narrative\u2014after all, everyone depends on NVIDIA. But disclosing that your models might fail, might be biased, might violate IP law? That challenges the core narrative. So companies stay silent.\n\nThe problem, of course, is that securities law doesn't care about your preferred narrative. It cares about material risks. And if you're more worried about dampening investor enthusiasm than about providing adequate disclosure, you've got your priorities exactly backwards.\n\n---\n\n### The Bottom Line: Advertising Capabilities, Hiding Risks\n\nWhen we step back and look at the complete picture, a pattern emerges with remarkable clarity. Companies are enthusiastically adopting AI terminology in their public disclosures\u20141,484 mentions of LLMs and generative AI, 3,249 mentions of AI and machine learning\u2014while systematically avoiding disclosure of AI-specific risks. Infrastructure gets disclosed at high rates (58%). Generic AI terminology gets mentioned frequently (47%). But the distinctive risks most likely to result in liability\u2014hallucination, training data provenance, bias, regulatory compliance, model failure\u2014barely register.\n\nThis is the disclosure inversion: companies are advertising what makes them look innovative while hiding what makes them vulnerable. For S-1 filers in 2026, this pattern represents more than just inadequate disclosure. It represents a strategic mistake that could haunt you for years.\n\nYou cannot claim AI as a competitive advantage in your business description while providing only generic risk disclosure in Item 1A. You cannot tell investors \"We're at the forefront of LLM technology\" without also telling them \"LLMs can hallucinate, and we face IP litigation risk from our training data.\" You cannot say \"AI powers our decision-making\" without disclosing \"our AI may exhibit bias that could violate anti-discrimination law.\"\n\nThe companies that figure this out first\u2014that provide comprehensive, specific, substantive AI risk disclosure even when it's uncomfortable\u2014will have the strongest legal position when the inevitable wave of AI-related litigation and enforcement arrives. The companies that follow the current inadequate  disclosure norms will be defending securities fraud claims with generic risk factors that courts will likely find insufficient.\n\nThe choice is yours. Lead with meaningful disclosure now, or defend inadequate disclosure later.\n\n\n---\n\n## III. The Cross-Jurisdictional Disclosure Landscape: Why EU Standards Create Competitive Advantage\n\n### When Regulation Becomes Strategy\n\nThe disclosure patterns we've identified in US SEC filings tell only part of the story. AI companies operate in a global regulatory environment where disclosure requirements vary dramatically by jurisdiction. Understanding these differences isn't just about compliance\u2014it's about competitive positioning and fiduciary duty.\n\nFor boards and investors, the key insight is this: companies that voluntarily adopt the **highest global standard** for AI disclosure signal superior governance, lower tail risk, and better institutional preparedness than competitors hiding behind minimal US requirements. In effect, disclosure regulation creates a separating equilibrium\u2014companies with genuine AI governance welcome rigorous disclosure, while companies with weak governance resist it.\n\n---\n\n### The United States: Flexible Framework, Weak Enforcement (For Now)\n\nThe SEC has not issued AI-specific disclosure requirements. Companies follow general materiality principles under Regulation S-K, which creates wide latitude for disclosure decisions. Our analysis shows this flexibility has produced a race to the bottom\u2014companies disclose infrastructure risks (safe, generic) while avoiding model-specific risks (material, uncomfortable).\n\nThe SEC has signaled increased scrutiny through \"AI washing\" enforcement actions against companies making misleading AI capability claims without corresponding risk disclosures. But formal guidance remains absent, creating uncertainty that most companies resolve by saying as little as possible.\n\n**Board implication**: If you're meeting only US minimum standards, you're likely underperforming global best practice. This creates both regulatory arbitrage risk (when SEC guidance tightens) and competitive disadvantage (when investors compare you to EU-compliant peers).\n\n---\n\n### The European Union: Mandatory AI Risk Disclosure Under the AI Act\n\nThe EU AI Act, in force since 2024, represents the most comprehensive AI governance framework globally. It imposes **mandatory disclosure requirements** for high-risk AI systems, including:\n\n**Specific disclosure obligations**: Companies must document training data provenance, bias testing methodologies, human oversight mechanisms, and incident response procedures. These aren't optional risk factors\u2014they're regulatory requirements with six-figure fines for non-compliance.\n\n**High-risk system classification**: Any AI system used for employment decisions, credit scoring, biometric identification, or critical infrastructure automatically qualifies as high-risk. This triggers enhanced disclosure, conformity assessment, and ongoing monitoring obligations.\n\n**Extraterritorial reach**: The Act applies to any company deploying AI systems in EU markets, regardless of where the company is headquartered. If you have European customers, you're subject to the Act.\n\n**The competitive dynamic**: US companies that proactively adopt EU-level disclosure standards in their SEC filings signal to global investors that they've already internalized the costs of compliance. Companies that disclose only for US markets create information asymmetry\u2014investors don't know if EU compliance costs are already embedded in projections or represent a future shock.\n\n---\n\n### The United Kingdom: Model Governance for Financial Services\n\nThe UK Financial Conduct Authority has pioneered sector-specific AI disclosure requirements through its \"model risk management\" framework. Financial institutions using AI for underwriting, fraud detection, or trading must disclose:\n\n**Model governance structure**: Who oversees model development, validation, and ongoing monitoring? What technical expertise sits on the board?\n\n**Performance metrics**: Actual accuracy rates, false positive/negative rates, and out-of-sample validation results. Generic claims of \"industry-leading accuracy\" won't suffice.\n\n**Incident disclosure**: Material model failures must be disclosed to regulators within defined timeframes. The FCA maintains a registry of AI-related incidents.\n\n**Why this matters globally**: The UK framework demonstrates that detailed, technical AI disclosure is feasible. Companies claiming they can't quantify model risks are revealing governance gaps, not technical limitations.\n\n---\n\n### China: Algorithmic Registration and Content Liability\n\nChina's Cyberspace Administration requires companies using \"algorithmic recommendation\" systems (essentially, any AI making personalized decisions) to register with the government and disclose:\n\n**Algorithm characteristics**: The fundamental logic and mechanism of the recommendation system must be documented and disclosed to regulators.\n\n**Data usage**: What data is collected, how it's processed, and whether it includes sensitive personal information.\n\n**Social impact**: Companies must assess and disclose the \"social responsibility\" implications of their algorithms, particularly for content moderation and information distribution.\n\nWhile China's regulatory framework serves different policy objectives than Western jurisdictions, it establishes the principle that governments can and will mandate granular AI disclosure. Companies operating in China who claim they can't provide technical details to US/EU investors are being inconsistent.\n\n---\n\n### Singapore: FEAT Principles Create Soft Law Standards\n\nThe Monetary Authority of Singapore's FEAT (Fairness, Ethics,  Accountability, Transparency) principles for AI in financial services aren't mandatory law\u2014they're supervisory expectations. But in practice, they function as disclosure standards because financial institutions compete on governance quality.\n\nThe FEAT framework requires disclosure of:\n\n**Fairness testing**: How do you test for algorithmic bias? What metrics do you use? What's your tolerance for demographic performance gaps?\n\n**Explainability**: Can you explain individual AI decisions in plain language? If not, why not, and what are the risk implications?\n\n**Accountability**: Who is personally accountable for AI system failures? This must be disclosed to regulators and, in many cases, to customers.\n\n**The disclosure dynamics**: Singapore's approach reveals what happens when regulators create **reputational incentives** for disclosure rather than legal mandates. In competitive markets, companies adopt higher standards to signal quality. In non-competitive markets (or where companies can coordinate), disclosure remains minimal.\n\n---\n\n### Canada: AIDA and the Emerging Compliance Framework\n\nCanada's Artificial Intelligence and Data Act (AIDA), part of Bill C-27, represents North America's first comprehensive AI-specific legislation. While still in legislative process, AIDA establishes disclosure obligations that Canadian companies and companies operating in Canadian markets should anticipate.\n\n**High-impact system designation**: AIDA requires companies deploying \"high-impact\" AI systems to notify the government and maintain detailed documentation. High-impact determination considers potential harm to health, safety, rights, or economic interests.\n\n**Transparency obligations**: Companies must disclose the use of AI systems to affected individuals in specific contexts, particularly where AI decisions significantly affect rights or opportunities. Generic privacy policy language won't suffice.\n\n**Impact assessment requirements**: For high-impact systems, companies must conduct and document impact assessments covering bias, accuracy, explainability, and human oversight. These assessments create a disclosure trail that could become relevant in securities filings for Canadian issuers.\n\n**The competitive dynamic for Toronto-Waterloo corridor**: Canada positions itself as an \"ethical AI\" leader, with companies in the Toronto-Waterloo tech corridor facing reputational pressure to exceed minimum AIDA standards. This creates interesting disclosure dynamics\u2014Canadian AI companies may voluntarily adopt more rigorous disclosure to signal alignment with national AI ethics positioning.\n\n**Cross-border implications**: For US companies with Canadian operations or customers, AIDA creates parallel compliance obligations to the EU AI Act. Companies disclosing only for US markets while operating in Canada face the same information asymmetry problem\u2014investors don't know if Canadian compliance costs are embedded or represent future shocks.\n\n---\n\n### The Disclosure Arbitrage Opportunity\n\nHere's what directors and investors need to understand: the global variation in AI disclosure standards creates both risk and opportunity.\n\n**The risk**: Companies can currently \"forum shop\" by incorporating disclosure decisions in their weakest jurisdiction. A Delaware corporation selling AI services globally can choose to disclose only to SEC standards (minimal) while remaining silent on EU Act obligations, UK model governance gaps, or China algorithmic registration requirements. When regulatory expectations converge\u2014and they will\u2014companies that minimized disclosure face large, sudden compliance costs.\n\n**The opportunity**: Companies that voluntarily adopt the **highest global standard** (currently the EU AI Act) for their US disclosures send a powerful market signal. They're telling investors: \"We've already internalized global compliance costs. We're not hiding regulatory arbitrage. Our governance is mature enough to welcome transparency.\"\n\nThis creates a separating equilibrium. Strong AI companies with genuine governance welcome rigorous disclosure because it differentiates them from weaker competitors. Weak AI companies resist disclosure because it would reveal governance gaps. Investors can use disclosure quality as a screening mechanism.\n\n---\n\n### The Fiduciary Duty Question for Boards\n\nDirectors approving minimal US-standard AI disclosure while the company operates under EU AI Act jurisdiction face a specific fiduciary duty question: Are you fulfilling your oversight responsibilities if material regulatory obligations aren't disclosed to investors?\n\nConsider this scenario: Your AI company operates in EU markets. The EU AI Act classifies your systems as high-risk, triggering compliance costs you estimate at $3-5M. Your S-1 makes generic statements about \"increasing AI regulation globally\" but doesn't mention the Act specifically, doesn't quantify compliance costs, and doesn't disclose that you're not currently compliant.\n\nSix months post-IPO, EU regulators fine you \u20ac10M for non-compliance. Shareholders sue, alleging inadequate disclosure. The plaintiff's attorney asks at deposition: \"Did the board know about the EU AI Act when approving the S-1?\" The answer is yes\u2014it's public law. \"Did the board know the company wasn't compliant?\" Almost certainly yes, if compliance costs $3-5M. \"Then why did you disclose only generic regulatory risk?\"\n\nThat's the fiduciary duty question. And the answer can't be \"because our securities lawyers said minimal disclosure was legally sufficient.\" The business judgment rule protects decisions made with adequate information. It doesn't protect decisions to withhold material information from investors.\n\n---\n\n### The Bottom Line on Cross-Jurisdictional Disclosure\n\nFor boards evaluating AI disclosure decisions, the cross-jurisdictional landscape offers a simple competitive litmus test:\n\n**Ask your disclosure team**: \"Are we meeting EU AI Act disclosure standards in our US filings, even though we're not legally required to?\"\n\nIf the answer is no, ask why. If the reasoning is \"we don't want to disclose that information,\" you've learned something important about your governance maturity.\n\nIf the reasoning is \"we can't quantify those risks\" or \"our technical teams don't have that data,\" you've learned something even more important\u2014you're deploying AI systems in regulated markets without the governance infrastructure to understand and control your risks.\n\nEither answer suggests disclosure isn't your real problem. Governance is.\n\n---\n\n## IV. The Academic Framework: What MIT, Stanford, and NIST Say You Should Be Disclosing\n\nWhile I cannot access your specific Google Sheet, academic frameworks from MIT, Stanford HAI, NIST, and EU AIHLEG identify **seven core AI risk categories** that should appear in material disclosures:\n\n### The Seven Categories (Academic Consensus)\n\n| Risk Category | Academic Risk Level | Current Disclosure Rate | Gap |\n|---------------|-------------------|------------------------|-----|\n| **1. Model Performance & Reliability** | High | <1% | \u274c **Critical** |\n| **2. Training Data & IP** | High | <1% | \u274c **Critical** |\n| **3. Bias & Fairness** | High | 9% | \u26a0\ufe0f **Significant** |\n| **4. Privacy & Data Protection** | High | ~30% | \u26a0\ufe0f **Moderate** |\n| **5. Security & Adversarial Attacks** | High | ~30% | \u26a0\ufe0f **Moderate** |\n| **6. Regulatory Compliance** | Medium | <1% | \u274c **Critical** |\n| **7. Societal & Ethical Impacts** | Medium | <1% | \u274c **Critical** |\n\n### What This Means\n\nIf you're an AI-focused company filing an S-1 in 2026 and your risk factors don't address **all seven categories**, your disclosure is materially deficient compared to:\n- Academic consensus on AI risk\n- EU regulatory requirements (AI Act)\n- NIST AI Risk Management Framework\n- Likely SEC expectations going forward\n\n---\n\n## IV. The Liability Implications: Why the Gap Matters\n\n### A. Section 11 Liability (Untested Statements)\n\n**The Rule**: Section 11 of the Securities Act imposes strict liability for material misstatements or omissions in registration statements.\n\n**The Risk**: If your S-1 states \"We use AI to power our platform\" but fails to disclose:\n- Model hallucination risks\n- Training data IP exposure\n- Bias/discrimination risks\n- EU AI Act compliance obligations\n\n...and any of these risks materialize post-IPO, you have a **Section 11 problem**.\n\n**Example Scenario**:\n> *Your AI-powered insurance company goes public. S-1 says \"We use machine learning to process claims.\" No mention of hallucination risk, bias, or model accuracy limitations.*\n>\n> *Six months post-IPO, your AI approves claims incorrectly, denies valid claims due to demographic bias, and state regulators investigate.*\n>\n> *Plaintiffs' bar alleges: \"You told us AI made you better. You didn't tell us AI could fail systematically.\"*\n>\n> **Result**: Your generic \"we use AI\" language provides no safe harbor because you didn't disclose **specific, known AI risks**.\n\n### B. Section 10(b) / Rule 10b-5 (Forward-Looking Statements)\n\n**The Rule**: Applies to material misstatements or omissions in connection with securities purchases.\n\n**The Risk**: Many S-1s include forward-looking statements about AI capabilities:\n- \"Our AI engine delivers industry-leading accuracy\"\n- \"Our models are trained on proprietary datasets\"\n- \"We use AI to provide personalized recommendations\"\n\nIf these statements are **not accompanied by specific risk disclosures**, they may not receive PSLRA safe harbor protection.\n\n**The Test**: Courts ask:\n1. Was the forward-looking statement accompanied by **meaningful cautionary language**?\n2. Was the cautionary language **substantive and tailored** to the specific statement?\n\nGeneric \"AI poses risks\" language likely fails this test.\n\n### C. The \"Bespeaks Caution\" Doctrine\n\n**What It Is**: A judicial doctrine that says **sufficiently specific warnings** can negate misleading optimism in prospectus documents.\n\n**What It Requires**:\n- Risk disclosures must be **specific** to the technology\n- Must address **known** or **reasonably knowable** risks\n- Must be **substantive**, not boilerplate\n\n**Application to AI**:\n\n| Statement Type | Inadequate Disclosure | Adequate Disclosure |\n|----------------|----------------------|---------------------|\n| **\"We use AI to automate underwriting\"** | \"AI systems may have errors\" | \"Our models may exhibit bias against protected classes; hallucinate policy details; or fail to detect fraud, leading to regulatory action or material losses\" |\n| **\"Our LLM powers customer service\"** | \"AI may produce inaccurate outputs\" | \"LLMs may generate factually incorrect, defamatory, or IP-infringing content; we face litigation risk from NYT, Getty Images, and authors' guilds regarding training data\" |\n| **\"We use ML for credit decisioning\"** | \"Algorithms may be biased\" | \"Our models may violate ECOA, Fair Lending, or FCRA; we have not conducted disparate impact testing; regulatory enforcement could require model retraining at cost of $[X]M\" |\n\n### D. Regulatory Enforcement Risk\n\n**The Landscape**:\n- **EU AI Act**: Now in force. High-risk AI systems require specific disclosures and governance\n- **SEC AI Guidance**: Expected in 2026-2027 following AI washing enforcement actions\n- **State AI Laws**: California, Colorado, others implementing AI-specific disclosure requirements\n\n**The Exposure**:\nIf your S-1 fails to address EU AI Act compliance and you operate in EU markets, you have:\n1. **Securities liability** (inadequate risk disclosure)\n2. **EU regulatory liability** (fines up to 6% of global revenue)\n3. **Reputational damage** (\"They went public knowing they weren't compliant\")\n\n---\n\n## V. What This Means for Your S-1: Practical Recommendations\n\n### Recommendation 1: Treat AI Risk Like Cybersecurity Risk (Pre-2018)\n\n**Historical Parallel**: Before 2018, cybersecurity disclosures were generic. Post-SolarWinds, WannaCry, and SEC guidance, **specific** cybersecurity risk disclosure became standard.\n\n**AI is following the same trajectory**\u2014don't wait for the first major AI-liability case to update your disclosures.\n\n### Recommendation 2: Map Your AI Use Cases to Risk Categories\n\nCreate a disclosure matrix:\n\n| AI Use Case | Model Performance | Training Data/IP | Bias | Privacy | Security | Regulatory | Societal |\n|-------------|-------------------|------------------|------|---------|----------|-----------|----------|\n| Claims processing | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u26a0\ufe0f Consider |\n| Customer chatbot | \u2705 Disclose | \u2705 Disclose | \u26a0\ufe0f Consider | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u26a0\ufe0f Consider |\n| Fraud detection | \u2705 Disclose | \u26a0\ufe0f Consider | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u2705 Disclose | \u2b1c N/A |\n\n###Recommendation 3: Quantify Where Possible\n\n**Generic** (provides minimal protection):\n> \"We may face regulatory fines related to AI compliance.\"\n\n**Specific** (provides substantive safe harbor):\n> \"Under the EU AI Act, our customer-facing chatbot is classified as 'high-risk.' Full compliance requires implementing human oversight, bias testing, and documentation systems. We estimate compliance costs of $2-4M and ongoing annual costs of $500K-1M. Failure to comply could result in fines up to \u20ac30M or 6% of annual global revenue, whichever is higher.\"\n\n### Recommendation 4: Address Known Litigation\n\nIf you're in generative AI, **you know** about:\n- *New York Times v. OpenAI*\n- *Getty Images v. Stability AI*\n- *Authors Guild class actions*\n\n**Inadequate**:\n> \"We may face IP claims.\"\n\n**Required**:\n> \"Our models are trained on large datasets that may include copyrighted content. Publishers and rights holders have filed lawsuits against other AI companies alleging that training on copyrighted works constitutes infringement. Similar claims against us could require us to pay significant damages, retrain our models at a cost of $[X]M-[Y]M over [Z] months, or discontinue products representing [%] of revenue.\"\n\n### Recommendation 5: Coordinate with Technical Teams\n\n**Critical**: Your risk disclosures should reflect **actual technical limitations** of your models.\n\nIf your ML engineers can tell you:\n- \"Our model has 87% accuracy in production\"\n- \"We haven't tested for demographic bias\"\n- \"We use GPT-4 via API and don't control training data\"\n\n**Your S-1 must reflect this reality**, not aspirational language.\n\n---\n\n## VI. The Compliance Timeline: What to Do Now\n\n| Timing | Action Item | Owner |\n|--------|-------------|-------|\n| **3-6 months pre-IPO** | Conduct AI risk inventory across all use cases | CTO + Legal |\n| **3-6 months pre-IPO** | Engage external AI risk assessment (NIST RMF audit) | Legal + Board |\n| **3 months pre-IPO** | Draft substantive AI risk factor language for each category | Securities Counsel |\n| **2 months pre-IPO** | Quantify compliance costs, remediation timelines | CFO + Legal |\n| **1 month pre-IPO** | Technical review of risk disclosures with AI team | CTO + General Counsel |\n| **Pre-effective amendment** | Update disclosures if material AI incidents occur | Legal |\n\n---\n\n## VII. Conclusion: The Bottom Line for General Counsel\n\n**Current state**: Only **9%** of public companies disclose AI bias risks. Less than **1%** address training data/IP exposure, hallucination, or EU AI Act compliance.\n\n**Your exposure**: If you file an S-1 in 2026 with generic AI risk language while:\n- Operating high-risk AI systems\n- Facing known IP litigation risks\n- Subject to EU AI Act requirements\n- Making forward-looking statements about AI capabilities\n\n...you are **not protected** by existing disclosure norms.\n\n**The recommendation**: Treat AI risk disclosure like you would have treated cybersecurity disclosure in 2017\u2014before it became standard, but after it became obvious it would be required.\n\n**The choice**: Lead by setting a high standard for AI risk disclosure now, or defend inadequate disclosures in securities litigation later.\n\n---\n\n## Appendix: Sample S-1 Disclosure Language by Risk Category\n\nThe following sample language templates provide specific, substantive disclosure examples for the five most critical AI risk categories identified in our empirical analysis. These templates are designed to satisfy both SEC materiality requirements and best-practice standards from EU AI Act, NIST AI RMF, and academic frameworks.\n\n---\n\n### 1. Model Performance & Reliability Risks\n\n**Context**: For companies whose products or services rely on AI model outputs for core functionality.\n\n**Sample Disclosure**:\n\n> **Our AI Models May Produce Inaccurate, Incomplete, or Harmful Outputs That Could Result in Liability, Reputational Damage, and Loss of Customer Trust**\n>\n> Our platform relies on large language models and machine learning algorithms to [describe core use case: generate content, make recommendations, automate decisions, etc.]. These AI systems, including both proprietary models we have developed and third-party models we license, are inherently probabilistic and may produce outputs that are factually incorrect, incomplete, nonsensical, biased, offensive, or otherwise harmful.\n>\n> **Hallucination and Factual Errors**: Large language models can generate plausible-sounding but factually incorrect information\u2014a phenomenon known as \"hallucination.\" Despite our testing and quality assurance processes, our models may hallucinate [specific examples relevant to your use case: medical information, legal advice, financial data, product specifications]. If users rely on hallucinated outputs for important decisions, we could face product liability claims, professional malpractice allegations, or regulatory enforcement.\n>\n> **Model Accuracy Limitations**: Our models achieve approximately [X]% accuracy in production environments based on our internal testing. However, model performance degrades when confronted with edge cases, adversarial inputs, out-of-distribution data, or novel scenarios not represented in training data. We cannot guarantee that our models will perform at stated accuracy levels for all users, use cases, or time periods. Material declines in model accuracy could lead to customer churn, contract terminations, and revenue loss.\n>\n> **Reliability and Availability**: AI model inference depends on complex technical infrastructure including GPUs, specialized software, and cloud services. Model latency, downtime, or degraded performance due to infrastructure failures could render our products unusable, triggering service level agreement breaches and financial penalties.\n>\n> **Remediation Costs**: If significant model performance issues emerge post-deployment, remediation may require model retraining, architecture changes, or deployment rollbacks at substantial cost. We estimate that complete model retraining could cost between $[X]M and $[Y]M and require [Z] months, during which product quality and customer satisfaction would be materially impaired.\n>\n> **Insurance Limitations**: Our existing product liability and errors & omissions insurance may not adequately cover claims arising from AI-generated outputs, particularly novel theories of liability related to hallucination, bias, or automated decision-making. We may face uninsured losses or be unable to obtain adequate insurance at commercially reasonable rates.\n\n---\n\n### 2. Training Data & Intellectual Property Risk\n\n**Context**: For companies developing proprietary AI models trained on large-scale data, particularly scraped web content or licensed datasets.\n\n**Sample Disclosure**:\n\n> **Our AI Models Are Trained on Large Datasets That May Include Copyrighted, Proprietary, or Personally Identifiable Information, Exposing Us to Intellectual Property Litigation and Regulatory Enforcement**\n>\n> **Training Data Sources and Provenance**: Our AI models are trained on datasets comprising [describe: billions of web pages, licensed content, user-generated content, proprietary data]. A substantial portion of this training data was obtained through automated web scraping, data licensing agreements, or user uploads. We cannot verify that all training data was obtained with proper authorization, that all licensing terms were complied with, or that all copyrighted material was used in accordance with fair use or other legal exceptions.\n>\n> **Intellectual Property Litigation Risk**: Publishers, content creators, and rights holders have filed lawsuits against AI companies alleging that training AI models on copyrighted works constitutes copyright infringement. Notable pending cases include *New York Times v. OpenAI*, *Getty Images v. Stability AI*, and class actions by the Authors Guild. These cases assert theories of direct infringement, vicarious liability, removal of copyright management information, and violation of terms of service. Adverse rulings in these or similar cases could:\n>\n> - Result in statutory damages potentially reaching billions of dollars\n> - Require us to pay ongoing licensing fees to rights holders, fundamentally changing our cost structure\n> - Compel us to retrain models using only licensed or public domain data, which we estimate would cost $[X]M-[Y]M and require [Z] months\n> - Force removal of products from the market, representing [%] of current revenue\n> - Create precedent that makes our business model legally untenable\n>\n> **Fair Use Uncertainty**: We believe our use of copyrighted training data constitutes fair use under U.S. copyright law based on the transformative nature of machine learning and the non-expressive use of training data. However, no court has definitively ruled on this issue in the context of generative AI. If courts reject the fair use defense for AI training, our business model would require fundamental restructuring.\n>\n> **International IP Exposure**: Copyright and data protection laws vary by jurisdiction. The EU Copyright Directive includes text and data mining exceptions with opt-out provisions. Japan permits AI training on copyrighted works without authorization. The UK is considering similar exceptions. Our training practices may comply with law in some jurisdictions while violating law in others, creating complex cross-border liability exposure.\n>\n> **Model Outputs May Reproduce Protected Content**: Despite technical measures to prevent verbatim reproduction of training data, our models may occasionally generate outputs that substantially reproduce copyrighted works, trademarked content, or proprietary information. Such outputs could trigger direct infringement liability and expose us to injunctive relief, damages, and reputational harm.\n>\n> **Data Deletion and Right to Be Forgotten**: Individuals whose personal information or creative works appear in our training data may demand deletion under GDPR, CCPA, or other data protection laws. Model \"unlearning\"\u2014removing specific data points from trained models without complete retraining\u2014remains technically immature. We may be unable to comply with deletion requests without prohibitively expensive retraining.\n\n---\n\n### 3. Regulatory Compliance & Cross-Jurisdictional Risk\n\n**Context**: For companies operating AI systems in multiple jurisdictions, particularly those subject to EU AI Act, sectoral regulations, or emerging AI-specific laws.\n\n**Sample Disclosure**:\n\n> **We Are Subject to Rapidly Evolving and Inconsistent AI Regulations Across Multiple Jurisdictions, and Compliance Failures Could Result in Substantial Fines, Operational Restrictions, and Competitive Disadvantage**\n>\n> **EU AI Act Compliance**: The European Union's Artificial Intelligence Act became law in 2024 and imposes risk-based requirements on AI systems deployed in EU markets. Our [describe systems: customer-facing chatbot, automated underwriting system, biometric identification tool] are classified as \"high-risk\" under the Act, triggering requirements for:\n>\n> - Conformity assessment and CE marking before deployment\n> - Ongoing bias and fairness testing with documented methodologies\n> - Human oversight mechanisms with specified intervention capabilities\n> - Detailed technical documentation and record-keeping\n> - Incident reporting to national authorities within defined timeframes\n> - Transparency obligations requiring disclosure of AI use to affected individuals\n>\n> We estimate that achieving full EU AI Act compliance will cost $[X]M-[Y]M over the next [Z] months and require ongoing annual compliance costs of $[A]M-[B]M. We are not currently fully compliant with all requirements. Non-compliance could result in fines up to \u20ac30 million or 6% of annual global revenue, whichever is higher, as well as orders to cease deployment of non-compliant systems in EU markets representing [%] of revenue.\n>\n> **Extraterritorial Regulatory Reach**: The EU AI Act applies to any provider placing AI systems on the EU market, regardless of where the provider is established. We cannot avoid EU AI Act requirements by incorporating outside the EU or maintaining EU operations through intermediaries. Similarly, other jurisdictions including China, the UK, and Canada are implementing AI regulations with extraterritorial reach.\n>\n> **U.S. Sectoral Regulation**: While the U.S. lacks comprehensive AI legislation, our AI systems are subject to sector-specific regulations including:\n>\n> - **Equal Credit Opportunity Act (ECOA) and Fair Lending**: Our AI-powered credit decisioning may violate fair lending laws if models exhibit disparate impact on protected classes. We have not completed disparate impact testing.\n> - **Fair Credit Reporting Act (FCRA)**: If our AI models constitute \"consumer reports,\" we face FCRA compliance obligations including accuracy requirements, adverse action notices, and consumer rights to dispute.\n> - **Employment Law**: AI-assisted hiring tools may violate Title VII, EEOC guidance, or state laws (including New York City Local Law 144) requiring bias audits and notice.\n> - **Health Insurance Portability and Accountability Act (HIPAA)**: AI processing of protected health information must comply with HIPAA privacy and security requirements.\n>\n> **State AI Legislation**: California, Colorado, Connecticut, and other states have enacted or are considering AI-specific disclosure, testing, and liability requirements. Multi-state compliance creates complexity and may require jurisdiction-specific model versions.\n>\n> **China Algorithmic Regulation**: Our AI systems deployed in China must register with the Cyberspace Administration of China and comply with algorithmic recommendation regulations requiring disclosure of algorithm characteristics, data usage, and social impact assessments. We currently operate in China without full algorithmic registration, creating enforcement risk.\n>\n> **Regulatory Uncertainty**: AI regulation is evolving rapidly with inconsistent requirements across jurisdictions. We may face conflicting legal obligations (e.g., EU requirements for algorithm transparency vs. U.S. trade secret protection). Compliance costs may escalate as regulations mature. New regulations may render our current business model impractical or prohibited in key markets.\n\n---\n\n### 4. Infrastructure Dependencies & Third-Party Risk\n\n**Context**: For companies relying on specialized AI infrastructure providers (cloud compute, GPU access, model APIs).\n\n**Sample Disclosure**:\n\n> **Our Business Depends on Continued Access to Specialized AI Infrastructure and Third-Party Model Providers, and Loss of Access Could Materially Disrupt Our Operations**\n>\n> **GPU and Compute Dependencies**: Training and deploying our AI models requires access to specialized graphics processing units (GPUs), particularly NVIDIA H100 and A100 chips, which are in limited global supply. We currently:\n>\n> - Depend on [cloud provider] for [%] of our compute capacity, creating single-vendor concentration risk\n> - Face GPU allocation limits that constrain our ability to scale model capacity to meet customer demand\n> - Pay spot-market prices for compute that have ranged from $[X] to $[Y] per GPU-hour, creating financial volatility\n> - Compete for GPU access with well-capitalized competitors who may outbid us for constrained resources\n>\n> If our cloud provider experiences outages, capacity constraints, or chooses to prioritize other customers, we may be unable to serve our customers, process inference requests, or train improved models. Alternative GPU providers have months-long waitlists and require advance commitments we cannot afford.\n>\n> **Third-Party Model Dependencies**: We license foundational AI models from [provider: OpenAI, Anthropic, Google, etc.] via API for [describe use case]. These dependencies create multiple risks:\n>\n> - **Service Discontinuation**: Our third-party provider may discontinue API access, modify pricing to uneconomical levels, or impose usage restrictions that prevent our use case.\n> - **Model Changes**: Providers may update models in ways that degrade performance for our specific application, require prompt re-engineering, or introduce unwanted behaviors.\n> - **Competitive Conflicts**: Our model provider may enter our market as a competitor or provide superior access to our competitors.\n> - **Data Privacy**: Sending customer data to third-party model APIs creates privacy and confidentiality risks, particularly for regulated industries.\n> - **Compliance Attribution**: We remain legally responsible for outputs generated by third-party models, but cannot control model behavior, training data, or bias characteristics.\n>\n> **Vendor Lock-In and Switching Costs**: Migrating from one model provider or cloud infrastructure platform to another requires substantial re-engineering, retraining, and customer migration. We estimate that switching costs would range from $[X]M-[Y]M and require [Z] months, during which service quality would be materially degraded.\n>\n> **Data Center and Network Risks**: AI inference requires low-latency, high-bandwidth connectivity to data centers housing GPU infrastructure. Natural disasters, cyberattacks, power outages, or network failures affecting our primary data center regions could render our services unavailable. Our disaster recovery and geographic redundancy capabilities are limited by GPU availability constraints.\n\n---\n\n### 5. AI Talent & Key Personnel Risk\n\n**Context**: For companies whose competitive advantage depends on specialized AI expertise.\n\n**Sample Disclosure**:\n\n> **Our Success Depends on Attracting and Retaining Specialized AI Talent in an Intensely Competitive Market, and Loss of Key Personnel Could Materially Harm Our Technology Development and Competitive Position**\n>\n> **Specialized Talent Requirements**: Developing, training, and deploying state-of-the-art AI systems requires expertise in machine learning, deep learning architectures, large-scale distributed systems, and domain-specific applications. The global supply of individuals with these combined skills is extremely limited, and demand far exceeds supply.\n>\n> **Competitive Talent Market**: We compete for AI talent against well-capitalized technology companies (including Google, Meta, Amazon, Microsoft, and OpenAI), research institutions offering academic prestige, and well-funded AI startups offering equity upside. Many of these competitors offer compensation packages materially exceeding what we can afford, particularly for the most sought-after individuals with publications in top-tier venues or experience training frontier models.\n>\n> **Key Person Dependencies**: Our AI capabilities depend substantially on [X] key AI researchers and engineers, including [describe: Chief AI Officer, lead ML engineers, specialized domain experts]. Loss of any of these individuals could:\n>\n> - Delay product development timelines by [X] months\n> - Require costly knowledge transfer and re-staffing\n> - Result in loss of institutional knowledge about model architectures, training procedures, and troubleshooting\n> - Signal competitive weakness to customers, investors, and remaining employees\n>\n> These key personnel are not bound by long-term employment contracts and may resign at any time. We do not maintain key person insurance.\n>\n> **Non-Compete Limitations**: California law prohibits most employee non-compete agreements. Our key AI personnel can immediately join direct competitors, taking knowledge of our model architectures, training techniques, and product roadmap. While we maintain confidentiality and IP assignment agreements, preventing unauthorized use or disclosure of trade secrets is difficult to enforce.\n>\n> **Equity Compensation Challenges**: As a pre-IPO company, we compensate AI talent substantially with equity grants. Post-IPO, stock price volatility may make our compensation less competitive than private AI companies offering new hire grants at higher valuations, or public technology companies with more liquid equity.\n>\n> **Immigration Risk**: Approximately [%] of our AI team members are not U.S. citizens and work under H-1B, O-1, or other visa categories. Changes in immigration policy, visa processing delays, or visa denials could prevent us from retaining or hiring essential talent. Remote work arrangements with non-U.S. personnel create data residency, export control, and IP protection challenges.\n\n---\n\n## Methodology\n\n**Data**: 1,876 SEC 10-K filings (complete universe of available data)\n**Extraction**: Automated Item 1A (Risk Factors) text extraction\n**Analysis**: Keyword frequency analysis across 30+ AI-related terms\n**Framework Comparison**: Cross-referenced against NIST AI RMF, EU AI Act requirements, MIT/Stanford academic frameworks\n\n---\n\n*This analysis is provided for informational purposes only and does not constitute legal advice. Consult qualified securities counsel for company-specific guidance.*\n\n**Prepared by**: Tanya Matanda  \n**Date**: January 2026\n";
            const rendered = marked.parse(paperMarkdown);
            document.getElementById('paper-rendered').innerHTML = rendered;
        }

        window.addEventListener('load', function() {
            initializeCharts();
        });

        function initializeCharts() {
            // Custom Zaha Hadid-inspired chart colors
            const primaryGradient = ['#6366f1', '#8b5cf6', '#a855f7', '#c026d3', '#d946ef', '#ec4899', '#f43f5e', '#ef4444'];
            const chartDefaults = {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    legend: {
                        labels: {
                            color: '#e2e8f0',
                            font: {
                                size: 12,
                                weight: '600'
                            }
                        }
                    },
                    title: {
                        color: '#a78bfa',
                        font: {
                            size: 16,
                            weight: '700'
                        }
                    }
                },
                scales: {
                    x: {
                        ticks: { color: '#9ca3af' },
                        grid: { color: 'rgba(255, 255, 255, 0.05)' }
                    },
                    y: {
                        ticks: { color: '#9ca3af' },
                        grid: { color: 'rgba(255, 255, 255, 0.05)' }
                    }
                }
            };

            // Disclosure Chart
            const disclosureCtx = document.getElementById('disclosureChart').getContext('2d');
            new Chart(disclosureCtx, {
                type: 'bar',
                data: {
                    labels: ['Infrastructure', 'Generic AI', 'LLM/Gen AI', 'Algorithm', 'Bias', 'EU AI Act', 'Training Data', 'Hallucination'],
                    datasets: [{
                        label: '% of AI Disclosers',
                        data: [58, 47, 41, 25, 9, 7, 0.9, 0.5],
                        backgroundColor: primaryGradient,
                        borderColor: primaryGradient.map(c => c + 'ff'),
                        borderWidth: 2,
                        borderRadius: 8
                    }]
                },
                options: {
                    ...chartDefaults,
                    plugins: {
                        ...chartDefaults.plugins,
                        legend: { display: false },
                        title: {
                            display: true,
                            text: 'What Companies Are Disclosing (% of 654 AI Disclosers)',
                            ...chartDefaults.plugins.title
                        }
                    },
                    scales: {
                        ...chartDefaults.scales,
                        y: {
                            ...chartDefaults.scales.y,
                            beginAtZero: true,
                            max: 100,
                            ticks: {
                                ...chartDefaults.scales.y.ticks,
                                callback: function(value) { return value + '%'; }
                            }
                        }
                    }
                }
            });

            // Infrastructure Chart
            const infraCtx = document.getElementById('infrastructureChart').getContext('2d');
            new Chart(infraCtx, {
                type: 'doughnut',
                data: {
                    labels: ['Infrastructure Mentions', 'Other'],
                    datasets: [{
                        data: [2375, 4503],
                        backgroundColor: ['#6366f1', 'rgba(255, 255, 255, 0.1)'],
                        borderColor: ['#8b5cf6', 'rgba(255, 255, 255, 0.2)'],
                        borderWidth: 2
                    }]
                },
                options: {
                    ...chartDefaults,
                    plugins: {
                        ...chartDefaults.plugins,
                        legend: { position: 'bottom', labels: chartDefaults.plugins.legend.labels },
                        title: {
                            display: true,
                            text: 'Infrastructure Risk Mentions (2,375)',
                            ...chartDefaults.plugins.title
                        }
                    }
                }
            });

            // Model Risk Chart
            const modelCtx = document.getElementById('modelRiskChart').getContext('2d');
            new Chart(modelCtx, {
                type: 'doughnut',
                data: {
                    labels: ['Model Performance Mentions', 'Other'],
                    datasets: [{
                        data: [17, 6861],
                        backgroundColor: ['#ec4899', 'rgba(255, 255, 255, 0.1)'],
                        borderColor: ['#f43f5e', 'rgba(255, 255, 255, 0.2)'],
                        borderWidth: 2
                    }]
                },
                options: {
                    ...chartDefaults,
                    plugins: {
                        ...chartDefaults.plugins,
                        legend: { position: 'bottom', labels: chartDefaults.plugins.legend.labels },
                        title: {
                            display: true,
                            text: 'Model Performance Risk Mentions (<17)',
                            ...chartDefaults.plugins.title
                        }
                    }
                }
            });

            // Keyword Chart
            const keywordCtx = document.getElementById('keywordChart').getContext('2d');
            new Chart(keywordCtx, {
                type: 'bar',
                data: {
                    labels: ['compute', 'artificial intelligence', 'llm', 'data center', 'machine learning', 'generative ai', 'automation', 'algorithm', 'bias', 'autonomous', 'ai model', 'ai system', 'gpu', 'nvidia', 'eu ai act'],
                    datasets: [{
                        label: 'Total Mentions',
                        data: [1430, 1051, 1038, 799, 456, 446, 335, 312, 214, 173, 123, 91, 67, 56, 46],
                        backgroundColor: '#8b5cf6',
                        borderColor: '#a78bfa',
                        borderWidth: 2,
                        borderRadius: 6
                    }]
                },
                options: {
                    ...chartDefaults,
                    indexAxis: 'y',
                    plugins: {
                        ...chartDefaults.plugins,
                        legend: { display: false },
                        title: {
                            display: true,
                            text: 'Top 15 AI Keywords in Risk Factors (Total Mentions)',
                            ...chartDefaults.plugins.title
                        }
                    }
                }
            });

            // Depth Chart
            const depthCtx = document.getElementById('depthChart').getContext('2d');
            new Chart(depthCtx, {
                type: 'pie',
                data: {
                    labels: ['1-5 mentions (Minimal)', '6-20 mentions (Moderate)', '21-50 mentions (Substantial)', '50+ mentions (Comprehensive)'],
                    datasets: [{
                        data: [63, 28.9, 5.8, 2.3],
                        backgroundColor: ['#ef4444', '#f59e0b', '#8b5cf6', '#6366f1'],
                        borderColor: ['#f87171', '#fbbf24', '#a78bfa', '#818cf8'],
                        borderWidth: 2
                    }]
                },
                options: {
                    ...chartDefaults,
                    plugins: {
                        ...chartDefaults.plugins,
                        legend: { position: 'bottom', labels: chartDefaults.plugins.legend.labels },
                        title: {
                            display: true,
                            text: 'AI Disclosure Depth Distribution (% of 654 Companies)',
                            ...chartDefaults.plugins.title
                        }
                    }
                }
            });
        }
    </script>
</body>
</html>
