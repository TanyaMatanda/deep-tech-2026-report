# S-1 AI Risk Factor Language: What the Data Shows
## An Empirical Analysis of 1,876 SEC Filings + Sample Disclosure Templates

*For General Counsel & Securities Attorneys*

---

## Executive Summary

We analyzed the actual Item 1A (Risk Factors) text from **1,876 SEC 10-K filings** to understand how public companies are currently disclosing AI-related risks. This analysis provides empirical benchmarks for S-1 drafting and identifies disclosure gaps that IPO-bound AI companies should address.

### Key Findings

| Metric | Finding |
|--------|---------|
| Companies with AI disclosures | **654 (34.9%)** |
| Total AI-related mentions | **6,878** |
| Most common keyword | "compute" (1,430 mentions) |
| Companies citing EU AI Act | **46** |

> **Bottom Line:** Over one-third of public companies now include AI-related risk disclosures. For AI-focused IPO candidates, comprehensive AI risk disclosure is no longer optional—it's baseline.

---

## Part I: What Public Companies Are Actually Disclosing

### Keyword Frequency Analysis

We scanned 1,876 10-K filings for 30+ AI-related keywords. Here's what we found:

| Rank | Keyword | Mentions | % of Disclosers |
|------|---------|----------|-----------------|
| 1 | compute | 1,430 | 58% |
| 2 | artificial intelligence | 1,051 | 42% |
| 3 | llm | 1,038 | 41% |
| 4 | data center | 799 | 32% |
| 5 | machine learning | 456 | 18% |
| 6 | generative ai | 446 | 18% |
| 7 | automation | 335 | 13% |
| 8 | algorithm | 312 | 12% |
| 9 | bias | 214 | 9% |
| 10 | autonomous | 173 | 7% |

### Risk Category Breakdown

| Category | Mentions | % of Total |
|----------|----------|------------|
| **Core AI** (artificial intelligence, ML, LLM, generative AI) | 3,249 | 47% |
| **AI Infrastructure** (compute, data center, GPU, NVIDIA) | 2,375 | 35% |
| **AI Technology** (automation, algorithm, robotics) | 945 | 14% |
| **AI Risk** (bias, hallucination, AI safety) | 261 | 4% |
| **AI Regulation** (EU AI Act, AI compliance) | 48 | <1% |

### Critical Observation

**Infrastructure dominates, but specific AI risks are underrepresented.**

Companies are disclosing compute and infrastructure dependencies at high rates, but specific AI risks—model performance, hallucination, bias, regulatory compliance—appear in only 4% of disclosures. This represents a **disclosure gap** that S-1 filers should address proactively.

---

## Part II: Top Disclosers—Benchmark Language

The following companies had the most comprehensive AI risk disclosures. Their language provides useful templates:

### Top 10 Companies by AI Disclosure Depth

| Rank | Ticker | Company | AI Mentions | Primary Focus |
|------|--------|---------|-------------|---------------|
| 1 | IREN | IREN Ltd | 128 | AI/ML infrastructure |
| 2 | ONDS | Ondas Holdings | 107 | AI in product development |
| 3 | NVDA | NVIDIA | 89 | Full AI value chain |
| 4 | CXM | Sprinklr | 86 | AI in customer experience |
| 5 | IONQ | IonQ | 76 | Quantum + AI convergence |
| 6 | GCT | GigaCloud Technology | 71 | AI in logistics |
| 7 | LMND | Lemonade | 67 | AI in claims processing |
| 8 | DOCN | DigitalOcean | 58 | AI infrastructure demand |
| 9 | AMBA | Ambarella | 53 | AI in semiconductor |
| 10 | RBRK | Rubrik | 52 | AI in cybersecurity |

---

## Part III: Sample S-1 Disclosure Language

Based on our analysis of current disclosure practices, we recommend the following language for AI-focused S-1 filers. These templates address the most common risk categories.

---

### A. Model Performance & Reliability

**Current practice:** 9% of companies mention "bias" in risk factors; fewer than 1% address "hallucination" specifically.

**Recommended S-1 Language:**

> *Our AI systems may produce inaccurate, biased, or harmful outputs that could damage our reputation, expose us to liability, and adversely affect our business.*
>
> Our products rely on artificial intelligence and machine learning models that generate outputs based on training data and algorithmic processes. These models may produce outputs that are inaccurate, incomplete, misleading, or biased. We cannot guarantee that our AI systems will perform as expected in all circumstances, particularly in novel situations not well-represented in our training data.
>
> Large language models, including those we develop or incorporate into our products, may generate outputs that appear plausible but are factually incorrect—a phenomenon sometimes referred to as "hallucination." Users may rely on such outputs without independent verification, potentially leading to adverse consequences.

---

### B. Training Data & IP Risk

**Current practice:** Only 17 mentions of "training data" across 1,876 filings—significant underrepresentation given ongoing litigation.

**Recommended S-1 Language:**

> *We may face claims that our AI models were trained on data obtained without proper authorization, which could result in significant legal liability and remediation costs.*
>
> Our AI models are trained on large datasets that may include content created by third parties. Although we employ processes to identify and address potential intellectual property concerns in our training data, these processes may not identify all potentially problematic content. Third parties have asserted, and may continue to assert, that our training practices infringe their copyrights, trademarks, or other intellectual property rights.
>
> The legal framework governing AI training on third-party content is evolving and uncertain. If courts or regulators determine that our training practices require additional licensing or constitute infringement, we may be required to pay substantial damages, obtain licenses on unfavorable terms, retrain our models, or discontinue certain products.

---

### C. Regulatory Compliance (EU AI Act)

**Current practice:** 46 companies now mention "EU AI Act" specifically—a 10x increase from prior year.

**Recommended S-1 Language:**

> *The regulatory landscape for AI is rapidly evolving, and new laws or regulations could require significant changes to our products or business practices.*
>
> Governments worldwide are considering or implementing regulations specifically addressing artificial intelligence. The European Union's AI Act imposes risk-based requirements on AI systems that may apply to our products, including requirements for transparency, human oversight, and documentation. The United States is developing AI governance frameworks through executive action and potential legislation. Other jurisdictions are considering their own AI regulatory approaches.
>
> We cannot predict the final form of AI regulations or their application to our business. Compliance with new AI regulations could require significant expenditure, product modifications, operational changes, or geographic restrictions on our products. Our products may be classified as "high-risk" under the EU AI Act, subjecting us to additional compliance obligations.

---

### D. Infrastructure Dependencies

**Current practice:** "compute" (1,430 mentions) and "NVIDIA" (56 mentions) indicate widespread acknowledgment of infrastructure concentration.

**Recommended S-1 Language:**

> *We depend on a limited number of suppliers for critical AI infrastructure, and disruptions to these relationships could materially harm our business.*
>
> Our AI development and deployment depends on specialized computing chips (primarily from NVIDIA), cloud computing services, and other infrastructure components sourced from a limited number of suppliers. These suppliers may be unable to meet our demand, may increase prices, or may prioritize other customers. Supply chain disruptions, geopolitical restrictions on semiconductor trade, or deterioration of our supplier relationships could limit our ability to train and deploy AI models.
>
> We anticipate needing to make capital expenditures of approximately $[___] over the next [___] fiscal years to maintain our competitive position in AI infrastructure.

---

### E. Talent & Key Personnel

**Current practice:** Underrepresented in current disclosures despite intense AI talent competition.

**Recommended S-1 Language:**

> *Our success depends on retaining key AI research and engineering personnel in a highly competitive talent market.*
>
> Competition for qualified AI professionals is intense, and we compete for talent with large technology companies and well-funded startups that may offer greater compensation, equity, or other inducements. Our AI development depends on a limited number of key technical personnel, and the loss of these individuals could significantly impair our research progress and product development.
>
> We may be unable to replace departing personnel with individuals of comparable skill and experience. Increases in compensation necessary to retain key employees could materially increase our operating expenses.

---

## Part IV: Disclosure Gaps & Recommendations

### What's Missing from Current Disclosures

Based on our analysis, we identified significant gaps between emerging AI risks and current disclosure practices:

| Risk Area | Current Disclosure Rate | Assessment |
|-----------|------------------------|------------|
| Compute/infrastructure | 58% | ✅ Well-covered |
| General AI mention | 42% | ✅ Adequate |
| Generative AI/LLM | 41% | ✅ Emerging standard |
| Algorithm/automation | 25% | ⚠️ Could improve |
| Bias | 9% | ⚠️ Underrepresented |
| AI regulation | <1% | ❌ Significant gap |
| Training data/IP | <1% | ❌ Critical gap |
| Hallucination | <1% | ❌ Critical gap |
| AI safety | <1% | ❌ Critical gap |

### Recommendations for S-1 Filers

1. **Be specific, not generic.** Avoid boilerplate language that could apply to any technology company. Describe your actual AI capabilities, training approaches, and risk mitigation measures.

2. **Quantify where possible.** Include specific dollar amounts for compliance costs, compute spending, and remediation timelines.

3. **Address known incidents.** If you have experienced AI-related incidents (model failures, content moderation issues, data breaches), disclose them and your remediation steps.

4. **Reference specific regulations.** The EU AI Act is now law. Reference it specifically rather than using generic "regulatory risk" language.

5. **Distinguish AI risk types.** Separate model performance risks, training data/IP risks, regulatory risks, and infrastructure risks into distinct disclosure items.

6. **Coordinate with technical teams.** Risk disclosures should be informed by actual technical capabilities and limitations. Involve AI engineers in the disclosure drafting process.

---

## Part V: SEC Enforcement Trends

Recent SEC actions and guidance suggest increased scrutiny of AI-related disclosures:

- **Specificity requirements:** SEC has criticized generic risk factors that fail to describe company-specific circumstances
- **Forward-looking statements:** AI capability claims must have reasonable basis; avoid overpromising on future AI performance
- **Cybersecurity integration:** AI security risks should be coordinated with broader cybersecurity disclosures under new SEC rules
- **Material information:** If AI is material to your business model, inadequate AI risk disclosure may constitute a material omission

---

## Methodology

**Data Source:** SEC EDGAR 10-K filings
**Companies Analyzed:** 1,876 (complete universe of companies with available 10-K data)
**Extraction Method:** Automated Item 1A (Risk Factors) text extraction
**Keywords Analyzed:** 30+ AI-related terms across 5 categories
**Analysis Date:** January 2026
**Processing Time:** 867.9 seconds

---

## About This Analysis

This analysis was conducted by GovernanceIQ Research as part of our IPO Readiness Series. The full dataset, including company-by-company AI disclosure scores and sample language, is available for institutional subscribers.

**For questions or custom analysis requests, contact:** [Your contact info]

---

*This document is provided for informational purposes only and does not constitute legal advice. Consult qualified securities counsel for company-specific disclosure guidance.*

**Last Updated:** January 2026
